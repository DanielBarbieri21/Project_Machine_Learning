# Estrutura do Projeto

## VisÃ£o Geral da Estrutura

```
Projeto_Machine_Learning/
â”‚
â”œâ”€â”€ ğŸ“ data_ingestion/              # IngestÃ£o de dados
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ kafka/                   # Kafka producer/consumer
â”‚   â”‚   â”œâ”€â”€ producer.py
â”‚   â”‚   â””â”€â”€ consumer.py
â”‚   â””â”€â”€ ğŸ“ etl/                     # ETL batch
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ batch_ingestion.py
â”‚
â”œâ”€â”€ ğŸ“ data_storage/                # Armazenamento de dados
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ ğŸ“ data_processing/             # Processamento de dados
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ğŸ“ spark/                   # Apache Spark
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ batch_processing.py
â”‚       â””â”€â”€ streaming_processing.py
â”‚
â”œâ”€â”€ ğŸ“ ml/                          # Machine Learning
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py                    # Script de treinamento
â”‚   â”œâ”€â”€ ğŸ“ models/                  # Modelos ML
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ classification_model.py
â”‚   â”‚   â””â”€â”€ regression_model.py
â”‚   â””â”€â”€ ğŸ“ mlops/                   # MLOps
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ mlflow_tracking.py
â”‚
â”œâ”€â”€ ğŸ“ api/                         # APIs RESTful
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                      # AplicaÃ§Ã£o FastAPI principal
â”‚   â””â”€â”€ ğŸ“ routes/                  # Rotas da API
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ predictions.py
â”‚
â”œâ”€â”€ ğŸ“ orchestration/               # OrquestraÃ§Ã£o
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ğŸ“ dags/                    # Airflow DAGs
â”‚       â”œâ”€â”€ data_pipeline.py
â”‚       â””â”€â”€ ml_pipeline.py
â”‚
â”œâ”€â”€ ğŸ“ visualization/               # VisualizaÃ§Ã£o
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ğŸ“ dashboards/
â”‚       â””â”€â”€ ml_dashboard.py
â”‚
â”œâ”€â”€ ğŸ“ monitoring/                  # Monitoramento
â”‚   â”œâ”€â”€ ğŸ“ prometheus/
â”‚   â”‚   â””â”€â”€ prometheus.yml
â”‚   â””â”€â”€ ğŸ“ grafana/
â”‚       â”œâ”€â”€ ğŸ“ datasources/
â”‚       â”‚   â””â”€â”€ prometheus.yml
â”‚       â””â”€â”€ ğŸ“ dashboards/
â”‚           â””â”€â”€ default.json
â”‚
â”œâ”€â”€ ğŸ“ infrastructure/              # Infraestrutura
â”‚   â””â”€â”€ ğŸ“ docker/
â”‚       â””â”€â”€ api.Dockerfile
â”‚
â”œâ”€â”€ ğŸ“ tests/                       # Testes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â””â”€â”€ test_ml_models.py
â”‚
â”œâ”€â”€ ğŸ“ scripts/                     # Scripts utilitÃ¡rios
â”‚   â”œâ”€â”€ setup.sh
â”‚   â”œâ”€â”€ start_services.sh
â”‚   â”œâ”€â”€ stop_services.sh
â”‚   â””â”€â”€ run_tests.sh
â”‚
â”œâ”€â”€ ğŸ“ docs/                        # DocumentaÃ§Ã£o
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ development_guide.md
â”‚   â”œâ”€â”€ getting_started.md
â”‚   â””â”€â”€ api_documentation.md
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # DocumentaÃ§Ã£o principal
â”œâ”€â”€ ğŸ“„ LICENSE                      # LicenÃ§a MIT
â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md              # Guia de contribuiÃ§Ã£o
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md         # Este arquivo
â”œâ”€â”€ ğŸ“„ requirements.txt             # DependÃªncias Python
â”œâ”€â”€ ğŸ“„ docker-compose.yml           # ConfiguraÃ§Ã£o Docker Compose
â”œâ”€â”€ ğŸ“„ env.example                  # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ ğŸ“„ .gitignore                   # Arquivos ignorados pelo Git
â””â”€â”€ ğŸ“„ Makefile                     # Comandos Make
```

## DescriÃ§Ã£o dos MÃ³dulos

### 1. data_ingestion/
MÃ³dulo responsÃ¡vel pela ingestÃ£o de dados de diversas fontes.

**Componentes:**
- `kafka/`: Producer e Consumer Kafka para dados em tempo real
- `etl/`: ETL batch para ingestÃ£o de dados em lote (CSV, JSON, Parquet, databases)

### 2. data_storage/
ConfiguraÃ§Ãµes e utilitÃ¡rios para armazenamento de dados.

**Componentes:**
- `config.py`: ConfiguraÃ§Ãµes de conexÃ£o com bancos de dados e storage

### 3. data_processing/
Processamento de dados em batch e streaming.

**Componentes:**
- `spark/batch_processing.py`: Processamento batch com Spark
- `spark/streaming_processing.py`: Processamento streaming com Spark

### 4. ml/
Machine Learning: modelos, treinamento e MLOps.

**Componentes:**
- `models/`: Modelos de classificaÃ§Ã£o e regressÃ£o
- `mlops/`: Tracking de experimentos com MLflow
- `train.py`: Script principal de treinamento

### 5. api/
APIs RESTful para exposiÃ§Ã£o de serviÃ§os.

**Componentes:**
- `app.py`: AplicaÃ§Ã£o FastAPI principal
- `routes/`: Rotas da API (prediÃ§Ãµes, modelos, etc.)

### 6. orchestration/
OrquestraÃ§Ã£o de workflows com Apache Airflow.

**Componentes:**
- `dags/`: DAGs do Airflow (pipelines de dados e ML)

### 7. visualization/
Dashboards e visualizaÃ§Ãµes.

**Componentes:**
- `dashboards/ml_dashboard.py`: Dashboard Dash para mÃ©tricas ML

### 8. monitoring/
Monitoramento com Prometheus e Grafana.

**Componentes:**
- `prometheus/`: ConfiguraÃ§Ã£o do Prometheus
- `grafana/`: Dashboards e datasources do Grafana

### 9. infrastructure/
ConfiguraÃ§Ãµes de infraestrutura.

**Componentes:**
- `docker/`: Dockerfiles para containers

### 10. tests/
Testes unitÃ¡rios e de integraÃ§Ã£o.

**Componentes:**
- Testes para API, ingestÃ£o e modelos ML

### 11. scripts/
Scripts utilitÃ¡rios para setup e gerenciamento.

**Componentes:**
- `setup.sh`: Setup inicial do projeto
- `start_services.sh`: Inicia serviÃ§os Docker
- `stop_services.sh`: Para serviÃ§os Docker
- `run_tests.sh`: Executa testes

### 12. docs/
DocumentaÃ§Ã£o completa do projeto.

**Componentes:**
- `architecture.md`: Arquitetura do sistema
- `development_guide.md`: Guia de desenvolvimento
- `getting_started.md`: Guia de inÃ­cio rÃ¡pido
- `api_documentation.md`: DocumentaÃ§Ã£o da API

## Fluxo de Dados

1. **IngestÃ£o**: Dados sÃ£o ingeridos via Kafka (streaming) ou ETL (batch)
2. **Armazenamento**: Dados brutos sÃ£o salvos no Data Lake
3. **Processamento**: Spark processa dados (batch ou streaming)
4. **TransformaÃ§Ã£o**: Dados sÃ£o limpos e transformados
5. **ML**: Modelos sÃ£o treinados com dados processados
6. **Deploy**: Modelos sÃ£o disponibilizados via API
7. **VisualizaÃ§Ã£o**: Dashboards mostram mÃ©tricas e resultados
8. **Monitoramento**: Prometheus e Grafana monitoram o sistema

## Tecnologias por MÃ³dulo

- **IngestÃ£o**: Kafka, ETL (Pandas, SQLAlchemy)
- **Processamento**: Apache Spark (PySpark)
- **ML**: Scikit-learn, TensorFlow, PyTorch
- **APIs**: FastAPI
- **OrquestraÃ§Ã£o**: Apache Airflow
- **Monitoramento**: Prometheus, Grafana
- **VisualizaÃ§Ã£o**: Dash, Plotly
- **ContainerizaÃ§Ã£o**: Docker, Docker Compose
- **MLOps**: MLflow

## PrÃ³ximos Passos

1. Configure as variÃ¡veis de ambiente em `.env`
2. Execute `make setup` para configurar o ambiente
3. Execute `make run` para iniciar os serviÃ§os
4. Consulte `docs/getting_started.md` para exemplos

